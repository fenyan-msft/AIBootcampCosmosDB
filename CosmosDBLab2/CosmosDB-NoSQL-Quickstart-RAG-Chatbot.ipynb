{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build a RAG chatbot with the Azure Cosmos DB NoSQL API\n",
        "\n",
        "In this sample, we'll demonstrate how to build a RAG pattern application using a subset of the Movie Lens dataset. This sample will use: \n",
        "- **Azure Cosmos DB for NoSQL** with the Python SDK to perform vector search for RAG, store and retrieve chat history, and store the vectors of the chat history to use as a semantic cache. \n",
        "- **Azure OpenAI** to generate embeddings and completions.\n",
        "- **Gradio**, an open source Python package, to create a simple user interface to allow users to type in questions and display responses generated by a GPT model or served from the cache. The responses will also display an elapsed time so you can see the impact caching has on performance versus generating a response.\n",
        "\n",
        "References:\n",
        "- [Azure Cosmos DB for NoSQL Python Quickstart](https://learn.microsoft.com/azure/cosmos-db/nosql/quickstart-python?pivots=devcontainer-codespace)\n",
        "- [Azure Cosmos DB for NoSQL Vector Search](https://learn.microsoft.com/azure/cosmos-db/nosql/vector-search)\n"
      ],
      "metadata": {},
      "id": "f4979f4a-ddb2-455d-9d16-39432d16cac2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the conda environment\n",
        "\n",
        "Start a terminal from the CosmosDBLab2 folder and run the following commands **sequentially**. Respond `y` when asked\n",
        ":\n",
        "\n",
        "```\n",
        "conda create --name CosmosDB_env python=3.10\n",
        "\n",
        "conda activate CosmosDB_env\n",
        "\n",
        "pip install ipykernel\n",
        "\n",
        "python -m ipykernel install --user --name CosmosDB_env --display-name \"CosmosDB_env\"\n",
        "\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "In this notebook, select the CosmosDB_env kernel. You may need to close and reopen this notebook to select it"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "f24fbd59-a3a1-4cca-a1ac-bde7a2a3cd26"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up a Cosmos DB account  \n",
        "\n",
        "Create a Cosmos DB account as follows:  \n",
        "- **Subscription**: *Select your subscription*  \n",
        "- **Resource Group**: *Select/Create a resource group*  \n",
        "- **Account name**: *Enter a unique name. The name can contain only lowercase letters, numbers and the \"-\" character*  \n",
        "- **Availability Zones**: *Disable*\n",
        "- **Location**: *Select a location*  \n",
        "- **Capacity mode**: *Provisioned throughput*  \n",
        "- **Apply Free Tier Discount**: *Do Not Apply*  \n",
        "- **Limit total account throughput**: *Leave unchecked*  \n",
        "\n",
        "Once the account is deployed, enable Vector Search for NoSQL API. This is a preview feature. To enable it, go to **Settings**, **Features** and enable the feature. This will take a couple of minutes and there will be a notification when it is enabled. \n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "3350363e-ccb1-4c70-8672-d9caf5723a54"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Azure OpenAI resource and deploy models\n",
        "If you don't already have an Azure OpenAI resource, create one.  \n",
        "Then deploy  \n",
        "- An embeddings model. **This has to be a `text-3-embedding-large` model**\n",
        "- A completions model, `gpt-35-turbo\n",
        "`"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "e0ccbb4f-dbab-4780-a07d-ede71c39a87e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Your Client Connection\n",
        "A sample `env_file.env` file is in this folder. Populate it with the appropriate credentials for Azure Cosmos DB and Azure OpenAI where marked with <>. Do not change any of the other values.  \n",
        "Click the Save button."
      ],
      "metadata": {},
      "id": "220b6fac-9cb5-4d99-94eb-d43d8cb7af08"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries\n",
        "import time\n",
        "import json\n",
        "import uuid\n",
        "import urllib \n",
        "import ijson\n",
        "import zipfile\n",
        "from dotenv import dotenv_values\n",
        "from openai import AzureOpenAI\n",
        "from azure.core.exceptions import AzureError\n",
        "from azure.cosmos import PartitionKey, exceptions\n",
        "from time import sleep\n",
        "import gradio as gr\n",
        "from azure.cosmos import CosmosClient\n",
        "\n",
        "# Load configuration\n",
        "env_name = \"env_file.env\"\n",
        "config = dotenv_values(env_name)\n",
        "\n",
        "cosmos_conn = config['cosmos_uri']\n",
        "cosmos_key = config['cosmos_key']\n",
        "cosmos_database = config['cosmos_database_name']\n",
        "cosmos_collection = config['cosmos_collection_name']\n",
        "cosmos_vector_property = config['cosmos_vector_property_name']\n",
        "comsos_cache_db = config['cosmos_cache_database_name']\n",
        "cosmos_cache = config['cosmos_cache_collection_name']\n",
        "\n",
        "# Create the Azure Cosmos DB for NoSQL client\n",
        "cosmos_client = CosmosClient(url=cosmos_conn, credential=cosmos_key)\n",
        "\n",
        "openai_endpoint = config['openai_endpoint']\n",
        "openai_key = config['openai_key']\n",
        "openai_api_version = config['openai_api_version']\n",
        "openai_embeddings_deployment = config['openai_embeddings_deployment']\n",
        "openai_embeddings_dimensions = int(config['openai_embeddings_dimensions'])\n",
        "openai_completions_deployment = config['openai_completions_deployment']\n",
        "\n",
        "# Movies file url\n",
        "# storage_file_url = config['storage_file_url']\n",
        "\n",
        "# Create the OpenAI client\n",
        "openai_client = AzureOpenAI(azure_endpoint=openai_endpoint, api_key=openai_key, api_version=openai_api_version)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721934480443
        }
      },
      "id": "1224b921-17e3-49fd-8abb-63459eeb2c28"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Create a database and containers with vector policies\n",
        "\n",
        "Next, we will create the Cosmos DB Database, as well as the containers for movie data and the semantic cache.  \n",
        "\n",
        "When creating the containers, we specify the vector embedding and indexing policies, so we will create these first."
      ],
      "metadata": {},
      "id": "011ec9d5-8fe7-406f-85a7-027b84a5189f"
    },
    {
      "cell_type": "code",
      "source": [
        "db = cosmos_client.create_database_if_not_exists(cosmos_database)\n",
        "\n",
        "# Create the vector embedding policy to specify vector details\n",
        "vector_embedding_policy = {\n",
        "    \"vectorEmbeddings\": [ \n",
        "        { \n",
        "            \"path\":\"/\" + cosmos_vector_property,\n",
        "            \"dataType\":\"float32\",\n",
        "            \"distanceFunction\":\"dotproduct\",\n",
        "            \"dimensions\":openai_embeddings_dimensions\n",
        "        }, \n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create the vector index policy to specify vector details\n",
        "indexing_policy = {\n",
        "    \"vectorIndexes\": [ \n",
        "        {\n",
        "            \"path\": \"/\"+cosmos_vector_property, \n",
        "            \"type\": \"quantizedFlat\" \n",
        "        }\n",
        "    ]\n",
        "} \n",
        "\n",
        "# Create the data collection with vector index (note: this creates a container with 10000 RUs to allow fast data load)\n",
        "try:\n",
        "    movies_container = db.create_container_if_not_exists(id=cosmos_collection, \n",
        "                                                  partition_key=PartitionKey(path='/id'), \n",
        "                                                  indexing_policy=indexing_policy,\n",
        "                                                  vector_embedding_policy=vector_embedding_policy,\n",
        "                                                  offer_throughput=10000) \n",
        "    print('Container with id \\'{0}\\' created'.format(movies_container.id)) \n",
        "\n",
        "except exceptions.CosmosHttpResponseError: \n",
        "    raise \n",
        "\n",
        "# Create the cache collection with vector index\n",
        "try:\n",
        "    cache_container = db.create_container_if_not_exists(id=cosmos_cache, \n",
        "                                                  partition_key=PartitionKey(path='/id'), \n",
        "                                                  indexing_policy=indexing_policy,\n",
        "                                                  vector_embedding_policy=vector_embedding_policy,\n",
        "                                                  offer_throughput=1000) \n",
        "    print('Container with id \\'{0}\\' created'.format(cache_container.id)) \n",
        "\n",
        "except exceptions.CosmosHttpResponseError: \n",
        "    raise"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721934582307
        }
      },
      "id": "305807f8-0205-481a-9d71-e8b88b504019"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate embeddings from Azure OpenAI\n",
        "\n",
        "This function is used to vectorize the user input for the vector search. It uses the text-3-embedding-large model with `dimensions = 256`"
      ],
      "metadata": {},
      "id": "2b922fc8-49a4-4c19-82c2-331089e2b131"
    },
    {
      "cell_type": "code",
      "source": [
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "@retry(wait=wait_random_exponential(min=2, max=300), stop=stop_after_attempt(20))\n",
        "def generate_embeddings(text):\n",
        "    \n",
        "    response = openai_client.embeddings.create(\n",
        "        input=text,\n",
        "        model=openai_embeddings_deployment,\n",
        "        dimensions=openai_embeddings_dimensions\n",
        "    )\n",
        "    \n",
        "    embeddings = response.model_dump()\n",
        "    return embeddings['data'][0]['embedding']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721934593674
        }
      },
      "id": "7ccf697d-f12c-4205-8a93-114ff3c3c86e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data from the json file\n",
        "Extract the MovieLens dataset from the zip file in the DataSet folder. The embeddings have already been generated in this dataset."
      ],
      "metadata": {},
      "id": "116bda9e-204e-4c02-b478-a52ebda70971"
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the data file\n",
        "with zipfile.ZipFile(\"DataSet/MovieLens-4489-256D.zip\", 'r') as zip_ref: \n",
        "#    zip_ref.extractall(\"/Data\")\n",
        "    zip_ref.extractall(\"DataSet\")\n",
        "zip_ref.close()\n",
        "# Load the data file\n",
        "data =[]\n",
        "\n",
        "with open('DataSet/MovieLens-4489-256D.json', 'r') as d:\n",
        "    data = json.load(d)\n",
        "# View the number of documents in the data (4489)\n",
        "len(data) "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721934607006
        }
      },
      "id": "efc296c5-82e3-4fc1-bff5-ea62893341f8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store data in Azure Cosmos DB. \n",
        "Insert data into Azure Cosmos DB for NoSQL.\n",
        "If you encounter throttling (429 error), run the cell again. "
      ],
      "metadata": {},
      "id": "12b46d75-61a6-4480-9b45-56b1f7c41950"
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "async def insert_data():\n",
        "    start_time = time.time()  # Record the start time\n",
        "    \n",
        "    counter = 0\n",
        "    tasks = []\n",
        "    max_concurrency = 5  # Adjust this value to control the level of concurrency\n",
        "    semaphore = asyncio.Semaphore(max_concurrency)\n",
        "    print(\"Starting doc load, please wait...\")\n",
        "    \n",
        "    def upsert_item_sync(obj):\n",
        "        movies_container.upsert_item(body=obj)\n",
        "    \n",
        "    async def upsert_object(obj):\n",
        "        nonlocal counter\n",
        "        async with semaphore:\n",
        "            await asyncio.get_event_loop().run_in_executor(None, upsert_item_sync, obj)\n",
        "            # Progress reporting\n",
        "            counter += 1\n",
        "            if counter % 100 == 0:\n",
        "                print(f\"Sent {counter} documents for insertion into collection.\")\n",
        "    \n",
        "    for obj in data:\n",
        "        tasks.append(asyncio.create_task(upsert_object(obj)))\n",
        "    \n",
        "    # Run all upsert tasks concurrently within the limits set by the semaphore\n",
        "    await asyncio.gather(*tasks)\n",
        "    \n",
        "    end_time = time.time()  # Record the end time\n",
        "    duration = end_time - start_time  # Calculate the duration\n",
        "    print(f\"All {counter} documents inserted!\")\n",
        "    print(f\"Time taken: {duration:.2f} seconds ({duration:.3f} milliseconds)\")\n",
        "\n",
        "# Run the async function\n",
        "await insert_data()\n",
        " "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721934758422
        }
      },
      "id": "becc3ad5-851c-4d44-8f6d-52a368d87b83"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will define several functions for our application to\n",
        "- Perform vector search\n",
        "- Get recent chat history\n",
        "- Generate chat completions\n",
        "- Cache generated responses"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "a306db0b-fe69-4d4d-8c1b-abf83fb89d85"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Vector Search\n",
        "\n",
        "This defines a function for performing a vector search over the movies data and chat cache collections. The function takes a collection reference, array of vector embeddings, and optional similarity score to filter for top matches and number of results to return to filter further."
      ],
      "metadata": {},
      "id": "0fca5dc5-ece1-4990-9ad8-c8e307d9eed5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a vector search on the Cosmos DB container\n",
        "def vector_search(container, vectors, similarity_score=0.02, num_results=5):\n",
        "    # Execute the query\n",
        "    results = container.query_items(\n",
        "        query= '''\n",
        "        SELECT TOP @num_results  c.overview, VectorDistance(c.vector, @embedding) as SimilarityScore \n",
        "        FROM c\n",
        "        WHERE VectorDistance(c.vector,@embedding) > @similarity_score\n",
        "        ORDER BY VectorDistance(c.vector,@embedding)\n",
        "        ''',\n",
        "        parameters=[\n",
        "            {\"name\": \"@embedding\", \"value\": vectors},\n",
        "            {\"name\": \"@num_results\", \"value\": num_results},\n",
        "            {\"name\": \"@similarity_score\", \"value\": similarity_score}\n",
        "        ],\n",
        "        enable_cross_partition_query=True, populate_query_metrics=True)\n",
        "    results = list(results)\n",
        "    # Extract the necessary information from the results\n",
        "    formatted_results = []\n",
        "    for result in results:\n",
        "        score = result.pop('SimilarityScore')\n",
        "        formatted_result = {\n",
        "            'SimilarityScore': score,\n",
        "            'document': result\n",
        "        }\n",
        "        formatted_results.append(formatted_result)\n",
        "\n",
        "    # #print(formatted_results)\n",
        "    metrics_header = dict(container.client_connection.last_response_headers)\n",
        "    #print(json.dumps(metrics_header,indent=4))\n",
        "    return formatted_results"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721934913248
        }
      },
      "id": "3dca9dde-5057-4f3f-a156-217ec148fe7f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get recent chat history\n",
        "\n",
        "This function provides conversational context to the LLM, allowing it to better have a conversation with the user."
      ],
      "metadata": {},
      "id": "f39630f4-ca2a-4240-8758-acf4135bbf1f"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chat_history(container, completions=3):\n",
        "    results = container.query_items(\n",
        "        query= '''\n",
        "        SELECT TOP @completions *\n",
        "        FROM c\n",
        "        ORDER BY c._ts DESC\n",
        "        ''',\n",
        "        parameters=[\n",
        "            {\"name\": \"@completions\", \"value\": completions},\n",
        "        ], enable_cross_partition_query=True)\n",
        "    results = list(results)\n",
        "    return results"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721934919055
        }
      },
      "id": "6c36729f-03eb-4d5a-9a14-1bf3da0388d8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Completion Functions\n",
        "Define the functions to handle the chat completion process, including caching responses. \n",
        "\n",
        "- `generate_completion` - this function assembles all of the required data as a payload to send to a GPT model to generate a completion\n",
        "\n",
        "- `chat_completion` - function defines the pipeline for our RAG Pattern application. When user submits a question, the cache is consulted first for an exact match. If no match then a vector search is made, chat history gathered, the LLM generates a response, which is then cached before returning to the user."
      ],
      "metadata": {},
      "id": "f34a36df-98a8-4aca-9fc3-617db7b3a9e8"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_completion(user_prompt, vector_search_results, chat_history):\n",
        "    \n",
        "    system_prompt = '''\n",
        "    You are an intelligent assistant for movies. You are designed to provide helpful answers to user questions about movies in your database.\n",
        "    You are friendly, helpful, and informative and can be lighthearted. Be concise in your responses, but still friendly.\n",
        "        - Only answer questions related to the information provided below. \n",
        "    '''\n",
        "\n",
        "    # Create a list of messages as a payload to send to the OpenAI Completions API\n",
        "\n",
        "    # system prompt\n",
        "    messages = [{'role': 'system', 'content': system_prompt}]\n",
        "\n",
        "    #chat history\n",
        "    for chat in chat_history:\n",
        "        messages.append({'role': 'user', 'content': chat['prompt'] + \" \" + chat['completion']})\n",
        "    \n",
        "    #user prompt\n",
        "    messages.append({'role': 'user', 'content': user_prompt})\n",
        "\n",
        "    #vector search results\n",
        "    for result in vector_search_results:\n",
        "        messages.append({'role': 'system', 'content': json.dumps(result['document'])})\n",
        "\n",
        "    print(\"Messages going to openai\", messages)\n",
        "    # Create the completion\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model = openai_completions_deployment,\n",
        "        messages = messages,\n",
        "        temperature = 0.1\n",
        "    )    \n",
        "    return response.model_dump()\n",
        "\n",
        "def chat_completion(cache_container, movies_container, user_input):\n",
        "    print(\"starting completion\")\n",
        "    # Generate embeddings from the user input\n",
        "    user_embeddings = generate_embeddings(user_input)\n",
        "    # Query the chat history cache first to see if this question has been asked before\n",
        "    cache_results = get_cache(container = cache_container, vectors = user_embeddings, similarity_score=0.99, num_results=1)\n",
        "    if len(cache_results) > 0:\n",
        "        print(\"Cached Result\\n\")\n",
        "        return cache_results[0]['completion'], True\n",
        "        \n",
        "    else:\n",
        "        #perform vector search on the movie collection\n",
        "        print(\"New result\\n\")\n",
        "        search_results = vector_search(movies_container, user_embeddings)\n",
        "\n",
        "        print(\"Getting Chat History\\n\")\n",
        "        #chat history\n",
        "        chat_history = get_chat_history(cache_container, 3)\n",
        "        #generate the completion\n",
        "        print(\"Generating completions \\n\")\n",
        "        completions_results = generate_completion(user_input, search_results, chat_history)\n",
        "\n",
        "        print(\"Caching response \\n\")\n",
        "        #cache the response\n",
        "        cache_response(cache_container, user_input, user_embeddings, completions_results)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        # Return the generated LLM completion\n",
        "        return completions_results['choices'][0]['message']['content'], False"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721937045907
        }
      },
      "id": "37b4569c-d1f5-4b82-a45d-552dca707385"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cache Generated Responses\n",
        "Save the user prompts and generated completions to the cache for faster future responses."
      ],
      "metadata": {},
      "id": "2e76a092-c184-44b8-a224-f7e57118147b"
    },
    {
      "cell_type": "code",
      "source": [
        "def cache_response(container, user_prompt, prompt_vectors, response):\n",
        "    # Create a dictionary representing the chat document\n",
        "    chat_document = {\n",
        "        'id':  str(uuid.uuid4()),  \n",
        "        'prompt': user_prompt,\n",
        "        'completion': response['choices'][0]['message']['content'],\n",
        "        'completionTokens': str(response['usage']['completion_tokens']),\n",
        "        'promptTokens': str(response['usage']['prompt_tokens']),\n",
        "        'totalTokens': str(response['usage']['total_tokens']),\n",
        "        'model': response['model'],\n",
        "        'vector': prompt_vectors\n",
        "    }\n",
        "    # Insert the chat document into the Cosmos DB container\n",
        "    container.create_item(body=chat_document)\n",
        "    print(\"item inserted into cache.\", chat_document)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721937050020
        }
      },
      "id": "ab546702-9725-4298-9edc-8a340f0603d1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a vector search on the Cosmos DB container\n",
        "def get_cache(container, vectors, similarity_score=0.0, num_results=5):\n",
        "    # Execute the query\n",
        "    results = container.query_items(\n",
        "        query= '''\n",
        "        SELECT TOP @num_results *\n",
        "        FROM c\n",
        "        WHERE VectorDistance(c.vector,@embedding) > @similarity_score\n",
        "        ORDER BY VectorDistance(c.vector,@embedding)\n",
        "        ''',\n",
        "        parameters=[\n",
        "            {\"name\": \"@embedding\", \"value\": vectors},\n",
        "            {\"name\": \"@num_results\", \"value\": num_results},\n",
        "            {\"name\": \"@similarity_score\", \"value\": similarity_score},\n",
        "        ],\n",
        "        enable_cross_partition_query=True, populate_query_metrics=True)\n",
        "    results = list(results)\n",
        "    #print(results)\n",
        "    return results"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721937051590
        }
      },
      "id": "e92ea0d5-c06a-4141-b600-f58668eadd7b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Simple Chat Bot UX in Gradio\n",
        "Build a user interface using Gradio for interacting with the AI application by clicking run on the cell.  \n",
        "Then click on the public URL link to chat with the bot about movies\n",
        "Ask questions such as \"Who are the characters in Toy Story?\"  \n",
        "To shut down the bot, interrupt the kernel and run the final cell."
      ],
      "metadata": {},
      "id": "182e30c5-07e1-40e8-8ac0-a0f35d472e5b"
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(label=\"Cosmic Movie Assistant\")\n",
        "    \n",
        "    msg = gr.Textbox(label=\"Ask me about movies in the Cosmic Movie Database!\")\n",
        "    clear = gr.Button(\"Clear\")\n",
        "\n",
        "    def user(user_message, chat_history):\n",
        "        # Create a timer to measure the time it takes to complete the request\n",
        "        start_time = time.time()\n",
        "        # Get LLM completion\n",
        "        response_payload, cached = chat_completion(cache_container, movies_container, user_message)\n",
        "        # Stop the timer\n",
        "        end_time = time.time()\n",
        "        elapsed_time = round((end_time - start_time) * 1000, 2)\n",
        "        response = response_payload\n",
        "        print(response_payload)\n",
        "        # Append user message and response to chat history\n",
        "        details = f\"\\n (Time: {elapsed_time}ms)\"\n",
        "        if cached:\n",
        "            details += \" (Cached)\"\n",
        "        chat_history.append([user_message, response_payload + details])\n",
        "        \n",
        "        return gr.update(value=\"\"), chat_history\n",
        "    \n",
        "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
        "\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "# Launch the Gradio interface\n",
        "demo.launch(debug=True,share=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721937279546
        }
      },
      "id": "b09f6295-092f-42a2-a063-e370d8f909f0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Be sure to run this cell to close or restart the Gradio demo\n",
        "demo.close()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721937283537
        }
      },
      "id": "218f2d62-5e42-4aa9-9583-92520a0691b3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning up - **IMPORTANT**\n",
        "Delete the Cosmos DB resource.  \n",
        "If you would like to keep it, **reduce the throughput**\n",
        " on both containers to 400 RU/s (under Scale and Settings) "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "21a45c3f-f5b0-4190-a6f4-896e5178e30d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "cosmosdb_env",
      "language": "python",
      "display_name": "CosmosDB_env"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "cosmosdb_env"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}